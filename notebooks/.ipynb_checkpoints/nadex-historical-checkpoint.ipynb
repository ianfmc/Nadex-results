{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d6e3fa-f558-4b09-8790-2c2770b841d3",
   "metadata": {},
   "source": [
    "**Install Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8945b2a-310c-4dc1-aaa6-edd8b2558d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --quiet\n",
    "!pip install tabulate --quiet\n",
    "!pip install pdfplumber --quiet\n",
    "!pip install tqdm\n",
    "\n",
    "!pip install pymupdf --quiet --upgrade --prefer-binary --only-binary :all:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae9d84-1a59-4dfd-a5d8-5d918dc07a9b",
   "metadata": {},
   "source": [
    "**Define the SkipFile exception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2d1a53-6892-4eee-a52b-b6c322419cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFile(Exception):\n",
    "    \"\"\"Raised to indicate this PDF should be skipped (no data to process).\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b2fd9-74b7-40c3-865c-1ab161a2da19",
   "metadata": {},
   "source": [
    "**Define page-finder & table-extractor functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b40a4f3-5ad6-4234-8438-5c0e1616ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fitz\n",
    "import pdfplumber\n",
    "\n",
    "def find_pages(pdf_path: str, keyword: str) -> list[int]:\n",
    "    \"\"\"\n",
    "    Return 1-based page numbers where `keyword` appears in the page text.\n",
    "    \"\"\"\n",
    "    pages = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for i in range(doc.page_count):\n",
    "            if keyword in doc[i].get_text():\n",
    "                pages.append(i + 1)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def extract_tables(\n",
    "    pdf_path: str,\n",
    "    pages: list[int],\n",
    "    period: str,\n",
    "    drop_type: str = \"Spread\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    From the given pages, pull out every table row where\n",
    "    df['Period']==period AND df['Type']!=drop_type.\n",
    "    Returns one concatenated DataFrame, or raises SkipFile if none.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for pnum in pages:\n",
    "            tbl = pdf.pages[pnum - 1].extract_table()\n",
    "            if not tbl:\n",
    "                continue\n",
    "            df = pd.DataFrame(tbl[1:], columns=tbl[0])\n",
    "            if \"Type\" in df.columns:\n",
    "                df[\"Type\"] = df[\"Type\"].str.strip()\n",
    "                df = df[df[\"Type\"] != drop_type]\n",
    "            if \"Period\" in df.columns:\n",
    "                df = df[df[\"Period\"] == period]\n",
    "            if not df.empty:\n",
    "                dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        # nothing matched → skip this PDF\n",
    "        raise SkipFile(f\"No tables extracted\")\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f1cfc-fa7b-4589-8f77-11159aa8b624",
   "metadata": {},
   "source": [
    "**Define a function to determine if \"In the Money\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11df9b86-003f-4683-89fb-7685b9322e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_the_money(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 'Exp Value' to numeric;\n",
    "    Extract the numeric strike price from 'Display Name' (number following '>');\n",
    "    Compute 'Flag' as 1 if Exp Value > Strike Price, else 0;\n",
    "    Keep the 'exp time' column unchanged.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            **{\n",
    "                'Exp Value': lambda df: pd.to_numeric(df['Exp Value'], errors='coerce'),\n",
    "                'Strike Price': lambda df: (\n",
    "                    df['Display Name']\n",
    "                      .str.extract(r'>\\s*([\\d.]+)', expand=False)\n",
    "                      .pipe(pd.to_numeric, errors='coerce')\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        .assign(\n",
    "            Flag=lambda df: (df['Exp Value'] > df['Strike Price']).astype(int)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffa62a-2d12-4d41-a229-15ac58f06761",
   "metadata": {},
   "source": [
    "**Define the clean and rename function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1561923-5ae6-4fd3-bb21-1898d0fcdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_and_rename(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    to_drop = [\n",
    "        c for c in [\n",
    "            \"Period\", \"Display Name\",\n",
    "            \"Type\", \"Buyer\", \"Seller\"\n",
    "        ] if c in df.columns\n",
    "    ]\n",
    "    return (\n",
    "        df\n",
    "        .rename(columns={\n",
    "            \"Business Date\": \"Date\",\n",
    "            \"Flag\": \"In the Money\",\n",
    "        })\n",
    "        .drop_duplicates(\n",
    "            subset=[\"Date\", \"Exp Value\", \"Strike Price\", \"Ticker\"],\n",
    "            keep=\"first\",\n",
    "        )\n",
    "        .drop(columns=to_drop)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3b9cb-01f1-4578-aa3a-c8316d760965",
   "metadata": {},
   "source": [
    "**Define a function to substitue the Ticker for the name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92925eb1-7bec-46a7-a2b1-00a26f5c6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "def add_ticker_from_mapping(\n",
    "    df: pd.DataFrame,\n",
    "    mapping_file: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a mapping CSV with columns 'Display Name' and 'Ticker',\n",
    "    then add a 'Ticker' column to `df` by substring-matching 'Name' against each 'Display Name'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Source DataFrame containing a 'Name' column.\n",
    "    mapping_file : str or Path\n",
    "        Path to CSV with mapping of 'Display Name' to 'Ticker'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with an added 'Ticker' column.\n",
    "    \"\"\"\n",
    "    mapping_df = pd.read_csv(mapping_file)\n",
    "    \n",
    "    def lookup_ticker(name: str) -> pd.Series:\n",
    "        if pd.isna(name):\n",
    "            return pd.NA\n",
    "        for desc, ticker in zip(mapping_df['Display Name'], mapping_df['Ticker']):\n",
    "            if desc in name:\n",
    "                return ticker\n",
    "        return pd.NA\n",
    "\n",
    "    result = df.copy()\n",
    "    result['Ticker'] = result['Name'].apply(lookup_ticker).fillna({'Ticker': 'UNKNOWN'})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d398a-b853-42b4-b234-ea4c1205f83c",
   "metadata": {},
   "source": [
    "**Define a function to save the final result to an S3 CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47b4dfb4-799e-456b-b202-aba3369fd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import io\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def upload_df_to_s3(\n",
    "    df: pd.DataFrame,\n",
    "    s3_client,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload a DataFrame to S3 as CSV using your provided S3 client.\n",
    "    \"\"\"\n",
    "    bucket = bucket.strip()\n",
    "\n",
    "    # sanity‐check bucket exists\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket)\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"Could not access bucket '{bucket}': {e}\") from e\n",
    "\n",
    "    # ——— Use BytesIO instead of StringIO ————————————————\n",
    "    csv_buffer = io.BytesIO()\n",
    "    # write UTF-8–encoded bytes, not text\n",
    "    csv_buffer.write(df.to_csv(index=False).encode('utf-8'))\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    try:\n",
    "        s3_client.upload_fileobj(\n",
    "            Fileobj=csv_buffer,\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            ExtraArgs={\"ContentType\": \"text/csv\"},\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to upload CSV to s3://{bucket}/{key}: {e}\"\n",
    "        ) from e\n",
    "\n",
    "    # optional confirm\n",
    "    try:\n",
    "        meta = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        # print(f\"   → Confirmed upload: {meta.get('ContentLength')} bytes\")\n",
    "    except Exception:\n",
    "        print(\"   ⚠️ Could not confirm upload with head_object\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15ccff-7041-4b3f-a97c-77f8e0f4fdee",
   "metadata": {},
   "source": [
    "**Define function to avoid re-processing processed files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3b83cd1-677b-4b98-9188-32eb05e0c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3 \n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def load_manifest(\n",
    "    s3_client: boto3.client, \n",
    "    bucket_name: str, \n",
    "    manifest_key: str = \"manifests/processed_files.json\"\n",
    ") -> set[str]:\n",
    "    \"\"\"\n",
    "    Download and parse the JSON manifest of processed PDFs.\n",
    "    Returns a set of keys, or empty set if it doesn’t exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = s3_client.get_object(Bucket=bucket_name, Key=manifest_key)\n",
    "        return set(json.loads(resp[\"Body\"].read()))\n",
    "    except ClientError as e:\n",
    "        # If the object isn’t found, return empty set; else re-raise\n",
    "        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "            return set()\n",
    "        raise\n",
    "\n",
    "def save_manifest(\n",
    "    processed: set[str],\n",
    "    s3_client: boto3.client,\n",
    "    bucket_name: str,\n",
    "    manifest_key: str = \"manifests/processed_files.json\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload the updated manifest back to S3.\n",
    "    \"\"\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=manifest_key,\n",
    "        Body=json.dumps(list(processed)).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63caae7-9e41-490e-93a7-01aab5404c49",
   "metadata": {},
   "source": [
    "**Define a function to get all the files related to trading results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87d8af33-1d99-4cf8-b8b6-13ed7c691f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def list_nadex_trading_results(bucket, prefix: str = \"\") -> List[str]:\n",
    "    \"\"\"\n",
    "    List PDF keys in the given bucket under `prefix`\n",
    "    that contain 'tradingResults' and end with .pdf.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        obj.key\n",
    "        for obj in bucket.objects.filter(Prefix=prefix)\n",
    "        if \"tradingResults\" in obj.key and obj.key.lower().endswith(\".pdf\")\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1730e25-03ff-4f12-bc3d-9ba00bc4ce53",
   "metadata": {},
   "source": [
    "**Define functions to setup access to S3, and filter the list of pdf's returned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "612ddbef-2962-4a5b-9be0-80d8dffb75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "def create_s3_clients(\n",
    "    profile: str = \"default\", region: str = \"us-east-1\"\n",
    ") -> Dict[str, boto3.client]:\n",
    "    session = boto3.Session(profile_name=profile, region_name=region)\n",
    "    return {\n",
    "        \"public\": session.client(\n",
    "            \"s3\",\n",
    "            config=Config(signature_version=UNSIGNED),\n",
    "            region_name=region,\n",
    "        ),\n",
    "        \"private\": session.client(\"s3\"),\n",
    "        \"resource\": session.resource(\"s3\"),\n",
    "    }\n",
    "\n",
    "def get_bucket(resource: boto3.resource, name: str):\n",
    "    return resource.Bucket(name)\n",
    "\n",
    "def parse_key_date(key: str) -> date:\n",
    "    stem = Path(key).stem\n",
    "    return datetime.strptime(stem[:8], \"%Y%m%d\").date()\n",
    "\n",
    "def filter_new_pdfs(\n",
    "    keys: Iterable[str],\n",
    "    processed: Iterable[str],\n",
    "    start: date = date(2024, 1, 1),\n",
    "    end: date | None = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return keys between start & end (inclusive) that aren’t in processed.\n",
    "    \"\"\"\n",
    "    if end is None:\n",
    "        end = date.today()\n",
    "    return [\n",
    "        key\n",
    "        for key in keys\n",
    "        if start <= (d := parse_key_date(key)) <= end\n",
    "        and key not in processed\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1510c0-0368-461a-820c-9a1c1fdb4377",
   "metadata": {},
   "source": [
    "**Define a helper function to process each file**\n",
    "\n",
    "**Also, define a function to run the Pipeline**\n",
    "1. For each PDF in the folder:\n",
    "2. Read the PDF file\n",
    "3. Extract the Tables with Daily contracts\n",
    "4. Create a CSV\n",
    "5. Write the CSV to S3\n",
    "6. Log the PDF file as 'processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9adad833-5c80-4d1b-b192-d07a3280d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from typing import List\n",
    "\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _process_pdf(\n",
    "    pdf_key: str,\n",
    "    target: str,\n",
    "    mapping_file: Path,\n",
    "    public_s3,\n",
    "    private_s3,\n",
    "    bucket_name: str,\n",
    "    nadex_bucket_name: str,\n",
    "    tmp_dir: Path = Path(\"/tmp\"),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Download a single PDF, extract/transform, upload the CSV, and\n",
    "    print a success message. Raises on any step failure.\n",
    "    \"\"\"\n",
    "    local_pdf = tmp_dir / Path(pdf_key).name\n",
    "    public_s3.download_file(\n",
    "        Bucket=nadex_bucket_name,\n",
    "        Key=pdf_key,\n",
    "        Filename=str(local_pdf),\n",
    "    )\n",
    "\n",
    "    def _format_ctx(exc: Exception) -> str:\n",
    "        tb = traceback.extract_tb(exc.__traceback__)\n",
    "        fn, ln, _, text = tb[-1]\n",
    "        return f\"{fn}:{ln} -> {text.strip()}\"\n",
    "        \n",
    "    try:\n",
    "        pages = find_pages(str(local_pdf), target)\n",
    "        df = (\n",
    "            extract_tables(str(local_pdf), pages, target)\n",
    "            .pipe(find_in_the_money)\n",
    "            .pipe(add_ticker_from_mapping, mapping_file)\n",
    "            .pipe(clean_and_rename)\n",
    "        )\n",
    "    except SkipFile as sf:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        ctx = _format_ctx(e)\n",
    "        raise RuntimeError(\n",
    "            f\"[EXTRACT/TRANSFORM ERROR] '{pdf_key}' (pages={pages}) \"\n",
    "            f\"({ctx}): {e}\"\n",
    "        ) from e\n",
    "\n",
    "    try:\n",
    "        upload_df_to_s3(df, private_s3, bucket_name, f\"historical/{Path(pdf_key).stem.split('_', 1)[0]}_Historical.csv\")\n",
    "    except Exception as e:\n",
    "        ctx = _format_ctx(e)\n",
    "        raise RuntimeError(\n",
    "            f\"[UPLOAD ERROR] '{pdf_key}' → '{bucket_name}/{pdf_key}' \"\n",
    "            f\"({ctx}): {e}\"\n",
    "        ) from e\n",
    "\n",
    "def run_nadex_pipeline(\n",
    "    mapping_file: Path,\n",
    "    target: str,\n",
    "    bucket_name: str,\n",
    "    nadex_bucket_name: str,\n",
    "    start: date,\n",
    "    end: date,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the full Nadex PDF → CSV pipeline over a given date range.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mapping_file : Path to the CSV mapping file used to enrich extracted tables.\n",
    "    target : The keyword to locate pages within each PDF (e.g. \"Daily\").\n",
    "    bucket_name : Name of your own S3 bucket where resulting CSVs and the manifest are stored.\n",
    "    nadex_bucket_name : Name of the public Nadex S3 bucket from which PDFs are downloaded (unsigned).\n",
    "    start : Lower bound (inclusive) on PDF dates to process (parsed from filenames).\n",
    "    end : Upper bound (inclusive) on PDF dates to process.\n",
    "\n",
    "    Actions:\n",
    "    --------\n",
    "    1. Bootstraps three S3 interfaces:\n",
    "       - `public_s3`: an unsigned client to download Nadex PDFs.\n",
    "       - `private_s3`: a signed client for uploading CSVs & manifest.\n",
    "       - `s3_resource`: resource interface to enumerate bucket objects.\n",
    "    2. Constructs `buckets` dict with Bucket objects for both source (market) and destination.\n",
    "    3. Loads the JSON “processed files” manifest from your private bucket into a `processed` set.\n",
    "    4. Iterates over every PDF key in the Nadex (market) bucket:\n",
    "       a. Skips any key already in `processed`, accumulating in `skipped`.\n",
    "       b. Parses the date out of the filename and skips if outside `[start, end]`.\n",
    "       c. Calls `_process_pdf(...)` to:\n",
    "          • Download PDF locally,\n",
    "          • Extract & transform tables,\n",
    "          • Enrich with ticker mapping,\n",
    "          • Upload the resulting CSV back to your bucket.\n",
    "       d. On success, adds the key to `processed`; on exception, logs to `errors`.\n",
    "    5. After the loop finishes, writes the updated `processed` manifest back to S3.\n",
    "    6. Prints a summary of how many files were processed, skipped, and errored.\n",
    "\n",
    "    \"\"\"\n",
    "    number_processed = number_skipped = number_of_errors = 0\n",
    "    \n",
    "    clients = create_s3_clients()\n",
    "    public_s3 = clients[\"public\"]\n",
    "    private_s3 = clients[\"private\"]\n",
    "    s3_resource = clients[\"resource\"]\n",
    "    buckets = {\n",
    "        \"daily\":  get_bucket(s3_resource, bucket_name),\n",
    "        \"market\": get_bucket(s3_resource, nadex_bucket_name),\n",
    "    }\n",
    "\n",
    "    processed = load_manifest(private_s3, bucket_name)\n",
    "    errors: Dict[str, str]  = {}\n",
    "\n",
    "    # 1) List *all* PDFs in the Nadex bucket\n",
    "    all_keys = list_nadex_trading_results(buckets[\"market\"], prefix=\"\")\n",
    "\n",
    "    # 2) Filter those to your date window\n",
    "    date_range_keys = [\n",
    "        k for k in all_keys\n",
    "        if start <= parse_key_date(k) <= end\n",
    "    ]\n",
    "\n",
    "    # 3) Split into new vs skipped-within-date-range\n",
    "    new_keys = []\n",
    "    skipped: Dict[str, str] = {}\n",
    "    \n",
    "    for k in date_range_keys:\n",
    "        if k in processed:\n",
    "            skipped[k] = \"already processed\"\n",
    "            number_skipped += 1\n",
    "        else:\n",
    "            new_keys.append(k)\n",
    "            number_processed += 1\n",
    "\n",
    "    errors: Dict[str, str] = {}\n",
    "    \n",
    "    for pdf_key in tqdm(\n",
    "        new_keys,\n",
    "        desc=\"Processing PDFs\",\n",
    "        ascii=True,\n",
    "    ):\n",
    "\n",
    "    # ─── Main loop (single statement!) ────────────────────────────────────────\n",
    "        try:\n",
    "            _process_pdf(\n",
    "                pdf_key,\n",
    "                target,\n",
    "                mapping_file,\n",
    "                public_s3,\n",
    "                private_s3,\n",
    "                bucket_name,\n",
    "                nadex_bucket_name,\n",
    "            )\n",
    "            processed.add(pdf_key)\n",
    "        except SkipFile as sf:\n",
    "            skipped[pdf_key] = str(sf)\n",
    "            number_skipped += 1\n",
    "        except Exception as exc:\n",
    "            errors[pdf_key] = str(exc)\n",
    "            number_of_errors += 1\n",
    "\n",
    "    save_manifest(processed, private_s3, bucket_name)\n",
    "\n",
    "    print()\n",
    "    print(f\"Done — processed: {len(processed)} files\")\n",
    "    print(f\"       skipped:   {len(skipped)} files already processed\")\n",
    "    print(f\"       errors:    {len(errors)} failures\")\n",
    "    \n",
    "    # if errors:\n",
    "    #     print(\"\\n❗ Failed files and their errors:\")\n",
    "    #     for fname, err in errors.items():\n",
    "    #         print(f\"  • {fname}: {err}\")\n",
    "    # if skipped:\n",
    "    #     print(\"\\nℹ️  Skipped files and reasons:\")\n",
    "    #     for fname, reason in skipped.items():\n",
    "    #         print(f\"  • {fname}: {reason}\")\n",
    "\n",
    "    return {\n",
    "        'files_processed': number_processed,\n",
    "        'files_skipped': number_skipped,\n",
    "        'files_with_errors': number_of_errors \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c659b17-58b5-4a37-88c2-dde5f033ec22",
   "metadata": {},
   "source": [
    "**Record the Run Log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "798b7deb-190c-4394-84b8-620da0a61efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "with open('../configs/s3.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    \n",
    "# Run log helper (append a row to S3 CSV)\n",
    "import io, csv, datetime as dt\n",
    "from botocore.exceptions import ClientError  # comes with boto3\n",
    "\n",
    "RUNLOG_FIELDS = [\n",
    "    'date','start_time','end_time','status',\n",
    "    'files_processed','files_skipped','files_error',\n",
    "    'run_id','notes'\n",
    "]\n",
    "\n",
    "from botocore.config import Config\n",
    "\n",
    "session = boto3.Session(region_name=cfg.get('region'))\n",
    "private_s3 = session.client('s3')\n",
    "public_s3 = session.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "def append_runlog_s3(\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    *,\n",
    "    start_time=None,\n",
    "    status='success',\n",
    "    files_processed=0,\n",
    "    files_skipped=0,\n",
    "    files_error=0,\n",
    "    run_id='',\n",
    "    notes=''\n",
    "):\n",
    "    now = dt.datetime.now()\n",
    "    start = start_time or now\n",
    "\n",
    "    row = {\n",
    "        'date': now.date().isoformat(),\n",
    "        'start_time': start if isinstance(start, str) else start.isoformat(timespec='seconds'),\n",
    "        'end_time': now.isoformat(timespec='seconds'),\n",
    "        'status': status,\n",
    "        'files_processed': int(files_processed),\n",
    "        'files_skipped': int(files_skipped),\n",
    "        'files_error': int(files_error),\n",
    "        'run_id': run_id,\n",
    "        'notes': notes,\n",
    "    }\n",
    "\n",
    "    # Fetch existing (if present)\n",
    "    buf = io.StringIO()\n",
    "    need_header = False\n",
    "    try:\n",
    "        obj = private_s3.get_object(Bucket=bucket, Key=key)\n",
    "        buf.write(obj['Body'].read().decode('utf-8'))\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] in ('NoSuchKey', '404'):\n",
    "            need_header = True\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    if buf.tell() == 0:\n",
    "        need_header = True\n",
    "\n",
    "    writer = csv.DictWriter(buf, fieldnames=RUNLOG_FIELDS)\n",
    "    if need_header:\n",
    "        writer.writeheader()\n",
    "    if buf.getvalue() and not buf.getvalue().endswith(''):\n",
    "        buf.write('')\n",
    "    writer.writerow(row)\n",
    "\n",
    "    private_s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        Body=buf.getvalue().encode('utf-8'),\n",
    "        ContentType='text/csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bedcc3-71c1-483a-9f2c-fe6212a64e3c",
   "metadata": {},
   "source": [
    "**Run the pipeline & display**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "328250b5-caa5-433e-81a1-649d37357f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|##########| 60/60 [01:49<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ℹ️  Skipped files and reasons:\n",
      "  • 20250303_tradingResults.pdf: already processed\n",
      "  • 20250304_tradingResults.pdf: already processed\n",
      "  • 20250305_tradingResults.pdf: already processed\n",
      "  • 20250306_tradingResults.pdf: already processed\n",
      "  • 20250307_tradingResults.pdf: already processed\n",
      "  • 20250310_tradingResults.pdf: already processed\n",
      "  • 20250311_tradingResults.pdf: already processed\n",
      "  • 20250312_tradingResults.pdf: already processed\n",
      "  • 20250313_tradingResults.pdf: already processed\n",
      "  • 20250314_tradingResults.pdf: already processed\n",
      "  • 20250317_tradingResults.pdf: already processed\n",
      "  • 20250318_tradingResults.pdf: already processed\n",
      "  • 20250319_tradingResults.pdf: already processed\n",
      "  • 20250320_tradingResults.pdf: already processed\n",
      "  • 20250321_tradingResults.pdf: already processed\n",
      "  • 20250324_tradingResults.pdf: already processed\n",
      "  • 20250325_tradingResults.pdf: already processed\n",
      "  • 20250326_tradingResults.pdf: already processed\n",
      "  • 20250327_tradingResults.pdf: already processed\n",
      "  • 20250328_tradingResults.pdf: already processed\n",
      "  • 20250331_tradingResults.pdf: already processed\n",
      "  • 20250401_tradingResults.pdf: already processed\n",
      "  • 20250402_tradingResults.pdf: already processed\n",
      "  • 20250403_tradingResults.pdf: already processed\n",
      "  • 20250404_tradingResults.pdf: already processed\n",
      "  • 20250407_tradingResults.pdf: already processed\n",
      "  • 20250408_tradingResults.pdf: already processed\n",
      "  • 20250409_tradingResults.pdf: already processed\n",
      "  • 20250410_tradingResults.pdf: already processed\n",
      "  • 20250411_tradingResults.pdf: already processed\n",
      "  • 20250414_tradingResults.pdf: already processed\n",
      "  • 20250415_tradingResults.pdf: already processed\n",
      "  • 20250416_tradingResults.pdf: already processed\n",
      "  • 20250417_tradingResults.pdf: already processed\n",
      "  • 20250421_tradingResults.pdf: already processed\n",
      "  • 20250422_tradingResults.pdf: already processed\n",
      "  • 20250423_tradingResults.pdf: already processed\n",
      "  • 20250424_tradingResults.pdf: already processed\n",
      "  • 20250425_tradingResults.pdf: already processed\n",
      "  • 20250428_tradingResults.pdf: already processed\n",
      "  • 20250429_tradingResults.pdf: already processed\n",
      "  • 20250430_tradingResults.pdf: already processed\n",
      "  • 20250501_tradingResults.pdf: already processed\n",
      "  • 20250502_tradingResults.pdf: already processed\n",
      "  • 20250505_tradingResults.pdf: already processed\n",
      "  • 20250506_tradingResults.pdf: already processed\n",
      "  • 20250507_tradingResults.pdf: already processed\n",
      "  • 20250508_tradingResults.pdf: already processed\n",
      "  • 20250509_tradingResults.pdf: already processed\n",
      "  • 20250512_tradingResults.pdf: already processed\n",
      "  • 20250513_tradingResults.pdf: already processed\n",
      "  • 20250514_tradingResults.pdf: already processed\n",
      "  • 20250515_tradingResults.pdf: already processed\n",
      "  • 20250516_tradingResults.pdf: already processed\n",
      "  • 20250519_tradingResults.pdf: already processed\n",
      "  • 20250520_tradingResults.pdf: already processed\n",
      "  • 20250521_tradingResults.pdf: already processed\n",
      "  • 20250522_tradingResults.pdf: already processed\n",
      "  • 20250523_tradingResults.pdf: already processed\n",
      "  • 20250526_tradingResults.pdf: already processed\n",
      "  • 20250527_tradingResults.pdf: already processed\n",
      "  • 20250528_tradingResults.pdf: already processed\n",
      "  • 20250529_tradingResults.pdf: already processed\n",
      "  • 20250530_tradingResults.pdf: already processed\n",
      "  • 20250602_tradingResults.pdf: already processed\n",
      "  • 20250603_tradingResults.pdf: already processed\n",
      "  • 20250604_tradingResults.pdf: already processed\n",
      "  • 20250605_tradingResults.pdf: already processed\n",
      "  • 20250606_tradingResults.pdf: already processed\n",
      "  • 20250609_tradingResults.pdf: already processed\n",
      "  • 20250610_tradingResults.pdf: already processed\n",
      "  • 20250611_tradingResults.pdf: already processed\n",
      "  • 20250612_tradingResults.pdf: already processed\n",
      "  • 20250613_tradingResults.pdf: already processed\n",
      "  • 20250616_tradingResults.pdf: already processed\n",
      "  • 20250617_tradingResults.pdf: already processed\n",
      "  • 20250618_tradingResults.pdf: already processed\n",
      "  • 20250619_tradingResults.pdf: already processed\n",
      "  • 20250620_tradingResults.pdf: already processed\n",
      "  • 20250623_tradingResults.pdf: already processed\n",
      "  • 20250624_tradingResults.pdf: already processed\n",
      "  • 20250625_tradingResults.pdf: already processed\n",
      "  • 20250626_tradingResults.pdf: already processed\n",
      "  • 20250627_tradingResults.pdf: already processed\n",
      "  • 20250630_tradingResults.pdf: already processed\n",
      "  • 20250701_tradingResults.pdf: already processed\n",
      "  • 20250702_tradingResults.pdf: already processed\n",
      "  • 20250703_tradingResults.pdf: already processed\n",
      "  • 20250707_tradingResults.pdf: already processed\n",
      "  • 20250708_tradingResults.pdf: already processed\n",
      "  • 20250709_tradingResults.pdf: already processed\n",
      "  • 20250710_tradingResults.pdf: already processed\n",
      "  • 20250711_tradingResults.pdf: already processed\n",
      "  • 20250714_tradingResults.pdf: already processed\n",
      "  • 20250715_tradingResults.pdf: already processed\n",
      "  • 20250716_tradingResults.pdf: already processed\n",
      "  • 20250717_tradingResults.pdf: already processed\n",
      "  • 20250718_tradingResults.pdf: already processed\n",
      "  • 20250721_tradingResults.pdf: already processed\n",
      "  • 20250722_tradingResults.pdf: already processed\n",
      "  • 20250723_tradingResults.pdf: already processed\n",
      "  • 20250724_tradingResults.pdf: already processed\n",
      "  • 20250725_tradingResults.pdf: already processed\n",
      "  • 20250728_tradingResults.pdf: already processed\n",
      "  • 20250729_tradingResults.pdf: already processed\n",
      "  • 20250730_tradingResults.pdf: already processed\n",
      "  • 20250731_tradingResults.pdf: already processed\n",
      "  • 20250801_tradingResults.pdf: already processed\n",
      "  • 20250804_tradingResults.pdf: already processed\n",
      "  • 20250805_tradingResults.pdf: already processed\n",
      "  • 20250806_tradingResults.pdf: already processed\n",
      "  • 20250807_tradingResults.pdf: already processed\n",
      "  • 20250808_tradingResults.pdf: already processed\n",
      "  • 20250811_tradingResults.pdf: already processed\n",
      "  • 20250812_tradingResults.pdf: already processed\n",
      "  • 20250813_tradingResults.pdf: already processed\n",
      "  • 20250814_tradingResults.pdf: already processed\n",
      "  • 20250815_tradingResults.pdf: already processed\n",
      "  • 20250818_tradingResults.pdf: already processed\n",
      "  • 20250819_tradingResults.pdf: already processed\n",
      "  • 20250820_tradingResults.pdf: already processed\n",
      "  • 20250821_tradingResults.pdf: already processed\n",
      "  • 20250822_tradingResults.pdf: already processed\n",
      "  • 20250825_tradingResults.pdf: already processed\n",
      "  • 20250826_tradingResults.pdf: already processed\n",
      "  • 20250827_tradingResults.pdf: already processed\n",
      "  • 20250828_tradingResults.pdf: already processed\n",
      "  • 20250829_tradingResults.pdf: already processed\n",
      "  • 20250901_tradingResults.pdf: already processed\n",
      "  • 20250902_tradingResults.pdf: already processed\n",
      "  • 20250903_tradingResults.pdf: already processed\n",
      "  • 20250904_tradingResults.pdf: already processed\n",
      "  • 20250905_tradingResults.pdf: already processed\n",
      "  • 20250908_tradingResults.pdf: already processed\n",
      "  • 20250909_tradingResults.pdf: already processed\n",
      "  • 20250910_tradingResults.pdf: already processed\n",
      "  • 20250911_tradingResults.pdf: already processed\n",
      "  • 20250912_tradingResults.pdf: already processed\n",
      "  • 20250915_tradingResults.pdf: already processed\n",
      "  • 20250916_tradingResults.pdf: already processed\n",
      "  • 20250917_tradingResults.pdf: already processed\n",
      "  • 20250918_tradingResults.pdf: already processed\n",
      "  • 20250919_tradingResults.pdf: already processed\n",
      "  • 20250301_tradingResults.pdf: No tables extracted\n",
      "  • 20250302_tradingResults.pdf: No tables extracted\n",
      "  • 20250308_tradingResults.pdf: No tables extracted\n",
      "  • 20250309_tradingResults.pdf: No tables extracted\n",
      "  • 20250315_tradingResults.pdf: No tables extracted\n",
      "  • 20250316_tradingResults.pdf: No tables extracted\n",
      "  • 20250322_tradingResults.pdf: No tables extracted\n",
      "  • 20250323_tradingResults.pdf: No tables extracted\n",
      "  • 20250329_tradingResults.pdf: No tables extracted\n",
      "  • 20250330_tradingResults.pdf: No tables extracted\n",
      "  • 20250405_tradingResults.pdf: No tables extracted\n",
      "  • 20250406_tradingResults.pdf: No tables extracted\n",
      "  • 20250412_tradingResults.pdf: No tables extracted\n",
      "  • 20250413_tradingResults.pdf: No tables extracted\n",
      "  • 20250418_tradingResults.pdf: No tables extracted\n",
      "  • 20250419_tradingResults.pdf: No tables extracted\n",
      "  • 20250420_tradingResults.pdf: No tables extracted\n",
      "  • 20250426_tradingResults.pdf: No tables extracted\n",
      "  • 20250427_tradingResults.pdf: No tables extracted\n",
      "  • 20250503_tradingResults.pdf: No tables extracted\n",
      "  • 20250504_tradingResults.pdf: No tables extracted\n",
      "  • 20250510_tradingResults.pdf: No tables extracted\n",
      "  • 20250511_tradingResults.pdf: No tables extracted\n",
      "  • 20250517_tradingResults.pdf: No tables extracted\n",
      "  • 20250518_tradingResults.pdf: No tables extracted\n",
      "  • 20250524_tradingResults.pdf: No tables extracted\n",
      "  • 20250525_tradingResults.pdf: No tables extracted\n",
      "  • 20250531_tradingResults.pdf: No tables extracted\n",
      "  • 20250601_tradingResults.pdf: No tables extracted\n",
      "  • 20250607_tradingResults.pdf: No tables extracted\n",
      "  • 20250608_tradingResults.pdf: No tables extracted\n",
      "  • 20250614_tradingResults.pdf: No tables extracted\n",
      "  • 20250615_tradingResults.pdf: No tables extracted\n",
      "  • 20250621_tradingResults.pdf: No tables extracted\n",
      "  • 20250622_tradingResults.pdf: No tables extracted\n",
      "  • 20250628_tradingResults.pdf: No tables extracted\n",
      "  • 20250629_tradingResults.pdf: No tables extracted\n",
      "  • 20250704_tradingResults.pdf: No tables extracted\n",
      "  • 20250705_tradingResults.pdf: No tables extracted\n",
      "  • 20250706_tradingResults.pdf: No tables extracted\n",
      "  • 20250712_tradingResults.pdf: No tables extracted\n",
      "  • 20250713_tradingResults.pdf: No tables extracted\n",
      "  • 20250719_tradingResults.pdf: No tables extracted\n",
      "  • 20250720_tradingResults.pdf: No tables extracted\n",
      "  • 20250726_tradingResults.pdf: No tables extracted\n",
      "  • 20250727_tradingResults.pdf: No tables extracted\n",
      "  • 20250802_tradingResults.pdf: No tables extracted\n",
      "  • 20250803_tradingResults.pdf: No tables extracted\n",
      "  • 20250809_tradingResults.pdf: No tables extracted\n",
      "  • 20250810_tradingResults.pdf: No tables extracted\n",
      "  • 20250816_tradingResults.pdf: No tables extracted\n",
      "  • 20250817_tradingResults.pdf: No tables extracted\n",
      "  • 20250823_tradingResults.pdf: No tables extracted\n",
      "  • 20250824_tradingResults.pdf: No tables extracted\n",
      "  • 20250830_tradingResults.pdf: No tables extracted\n",
      "  • 20250831_tradingResults.pdf: No tables extracted\n",
      "  • 20250906_tradingResults.pdf: No tables extracted\n",
      "  • 20250907_tradingResults.pdf: No tables extracted\n",
      "  • 20250913_tradingResults.pdf: No tables extracted\n",
      "  • 20250914_tradingResults.pdf: No tables extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'commit_sha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m run_id \u001b[38;5;241m=\u001b[39m run_start\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# After run: append run log with counters\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mappend_runlog_s3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBUCKET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRUNLOG_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuccess\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiles_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpartial\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiles_processed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfailed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_processed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiles_processed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_skipped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiles_skipped\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiles_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnotes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHistorical batch run\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 45\u001b[0m, in \u001b[0;36mappend_runlog_s3\u001b[0;34m(bucket, key, start_time, status, files_processed, files_skipped, files_error, run_id, notes)\u001b[0m\n\u001b[1;32m     34\u001b[0m now \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     35\u001b[0m start \u001b[38;5;241m=\u001b[39m start_time \u001b[38;5;129;01mor\u001b[39;00m now\n\u001b[1;32m     37\u001b[0m row \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: now\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;241m.\u001b[39misoformat(),\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m: start \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m start\u001b[38;5;241m.\u001b[39misoformat(timespec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m'\u001b[39m: now\u001b[38;5;241m.\u001b[39misoformat(timespec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: status,\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles_processed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(files_processed),\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles_skipped\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(files_skipped),\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles_error\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(files_error),\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommit_sha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcommit_sha\u001b[49m,\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotes\u001b[39m\u001b[38;5;124m'\u001b[39m: notes,\n\u001b[1;32m     47\u001b[0m }\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Fetch existing (if present)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'commit_sha' is not defined"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "with open('../configs/s3.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "BUCKET = cfg['bucket']\n",
    "PUBLIC_BUCKET = cfg['public_bucket']\n",
    "HIST_PREFIX = cfg['prefixes']['historical']\n",
    "MANIFEST_KEY = f\"{cfg['prefixes']['manifests']}/processed_files.json\"\n",
    "RUNLOG_KEY = f\"{cfg['prefixes']['logs']}/run_log.csv\"\n",
    "MAPPING_FILE = Path(cfg['mapping_file'])\n",
    "\n",
    "# Append a run-log row to S3 CSV\n",
    "import io, csv, datetime as dt\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "metrics = run_nadex_pipeline(\n",
    "    mapping_file=MAPPING_FILE,\n",
    "    target=\"Daily\",\n",
    "    bucket_name=BUCKET,\n",
    "    nadex_bucket_name=PUBLIC_BUCKET,\n",
    "    start=date(2025, 3, 1),\n",
    "    end=date.today(),\n",
    ")\n",
    "\n",
    "# Use config in the final run cell\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "\n",
    "run_start = dt.datetime.now()\n",
    "\n",
    "# Generate a run_id (timestamp) for provenance (no Git required)\n",
    "run_id = run_start.strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "# After run: append run log with counters\n",
    "append_runlog_s3(\n",
    "    BUCKET, RUNLOG_KEY,\n",
    "    start_time=run_start,\n",
    "    status=('success' if metrics.get('files_error', 0) == 0 else\n",
    "            'partial' if metrics.get('files_processed', 0) > 0 else 'failed'),\n",
    "    files_processed=metrics.get('files_processed', 0),\n",
    "    files_skipped=metrics.get('files_skipped', 0),\n",
    "    files_error=metrics.get('files_error', 0),\n",
    "    run_id=run_id,\n",
    "    notes='Historical batch run'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f21e6-e537-4cab-8756-1df8d62f0b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
